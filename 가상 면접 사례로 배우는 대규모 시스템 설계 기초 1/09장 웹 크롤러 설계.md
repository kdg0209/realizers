# 웹 크롤러 설계

- 당연하게도 시스템의 복잡도는 웹 크롤러가 처리해야하는 데이터의 규모에 따라 달라질 수 있습니다.

<br>

## 1단계. 문제 이해 및 설계 범위 확정

### 웹 크롤러의 기본 원리

1. URL 목록을 받아, 해당 URL들이 가리키는 모든 웹 페이지를 다운로드 합니다.
2. 다은받은 웹 페이지에서 URL을 추출합니다.
3. 추출한 URL을 다시 1번과정을 되풀이합니다.

![스크린샷 2024-06-27 오후 9 22 16](https://github.com/kdg0209/realizers/assets/80187200/0bf83fe6-bedc-46e3-b218-e18314c221f1)

<br>

### 주의사항

#### 규모 확장성

- 웹은 어마무시하게 거대합니다. 오늘날의 웹은 수십억개의 페이지가 등록되어 있고, 이를 처리하기 위해서는 병렬성을 활용해야 합니다.

#### 안정성

- 웹은 함정이 가득합니다. 잘못 작성된 HTML, 반응 없는 서버, 악성 코드가 붙어있는 링크들이 있기 때문에 이에 대비를 해야합니다.

#### 예절

- 크롤러는 수집 웹 사이트를 짧은 시간 동안 너무 많이 요청해서는 안됩니다.

#### 확장성

- 이미지 파일이나 동영상을 크롤링하고 싶다고해서 전체 시스템을 갈아엎으면 안됩니다. 처음부터 확장성 있게 설계해야합니다.

<br>

### 계략적 규모 추정

- 매달 10억개의 웹 페이지를 다운로드합니다.
- QPS = 10억 / 30일 / 24시간 / 3600초 = 대략 400초
- 웹 페이지의 평균 크기는 500k
- 10억 페이지 x 500k = 월 500TB
- 5년간 보관해야하는 용량 = 500TB x 12 x 5 = 30PB

<br>

## 2단계. 개략적 설계안 제시 및 동의 구하기

![스크린샷 2024-06-27 오후 9 40 35](https://github.com/kdg0209/realizers/assets/80187200/f0ac878f-048b-4b51-adc6-986a15c13698)

<br>

#### 시작 URL 목록

- 웹 크롤러가 크롤링을 시작하는 출발점입니다.
- 웹 크롤러가 가능한 많은 링크를 탐색할 수 있도록 하는 URL을 고릅니다. 나라별로 인기있는 웹 사이트(네이버 블로그, 뉴스, 티스토리 등), 주제별로 다른 사이트(쇼핑, 스포츠, 건강 등)

#### 미수집 URL 저장소

- 현대의 웹 크롤러는 크롤링 상태를 두가지로 나타내는데, 다운로드할 URL을 미수집 URL이라고 부릅니다.
  - 다운로드할 URL
  - 다운로드된 URL

#### HTML 다운로더

- HTML 다운로더는 인터넷에서 웹 페이지를 다운로드하는 컴포넌트입니다. 다운로드할 페이지의 URL은 미수집 URL 저장소가 제공합니다.

#### 도메인 이름 변환기

- 웹 페이지를 다운로드받기 위해서는 URL을 IP로 변환하는 절차가 필요합니다. HTML 다운로더는 도메인 이름 변환기를 사용하여 URL에 대응되는 IP주소를 알아냅니다.

#### 콘텐츠 파서

- 웹 페이지를 다운로드하면 파싱과 검증 절차를 거쳐야합니다. 그 이유는 악성코드로 인해 문제가 발생할 수 있기 때문입니다.

#### 중복 콘텐츠인가?

- 같은 데이터를 중복해서 저장할 수 있기 때문에 이 문제를 해결하기 위해 데이터베이스에 있는 콘텐츠와 방금 읽어들인 콘텐츠의 해시값을 비교하는 것입니다. 콘텐츠의 내용을 문자열로 비교할 수 있지만 문자열이 엄청 길어질때 비효율이기 때문입니다.

#### 콘텐츠 저장소

- 콘텐츠 저장소는 HTML 문서를 보관하는 시스템입니다.
- 저장소는 특성에 따라 RDBMS, NoSql을 선택할 수 있으며, 저장할 데이터의 크기, 유형, 검색 빈도, 낮은 레이턴시 등을 고려해서 선택합니다. (아마 NoSql?)

#### URL 추출기

- URL 추출기는 HTML 페이지를 파싱하여 링크를 골라내는 역할을 합니다.

#### URL 필터

- URL 필터는 특정한 content-type이나 파일 확장자를 갖는 URL, 접속 시 오류가 발생하는 URL 등을 크롤링 대상에서 제외시킵니다.

#### 이미 방문한 URL?

- 이미 방문한적이 있는 URL인지 추적하면 같은 URL을 여러번 처리하는 일을 방지할 수 있고, 무한 루프에 빠지는 일을 방지할 수 있습니다.

#### URL 저장소

- URL 저장소는 이미 방문한적이 있는 URL을 저장하는 보관소입니다.

<br>

## 3단계. 상세 설계

### DFS를 쓸 것인가, BFS를 쓸 것인가?

- 페이지는 노드이고, 링크는 엣지라고 보면 됩니다.
- DFS(깊이 우선 탐색법)은 좋은 선택지가 아닐 가능성이 높다고 합니다. 그 이유는 링크의 깊이가 A -> B -> C -> D ... 이렇게 깊어질 수 있기 때문입니다.

#### BFS(너비 우선 탐색법)

- 웹 크롤러는 너비 우선 탐색법을 사용한다고 하는데, BFS는 FIFO 큐를 사용하는 알고리즘이라 합니다. 하지만 BFS를 사용하더라도 두 가지 문제점이 있다고 합니다.

- 문제점 1
  - 한 페이지에서 나오는 링크의 상당수는 다시 같은 서버로 되돌아가는데, 이때 병령적으로 해당 서버에 요청을 하게되면 의도치않은 도스 공격이 될 수 있습니다.
- 문제점 2
  - BFS는 FIFO 큐를 사용하여 순서대로 처리를 하는데 이는 모든 것을 순서에 따라 공평하게 처리한다는 의미인데, 하지만 모든 웹 사이트가 같은 수준의 품질, 같은 수준의 중요성을 갖지 않습니다. 즉 우선순위를 여러가지 척도에 의해 구별되어야 하는데 FIFO는 이에 적합하지 않을 수 있습니다.

<br>

### 미수집 URL 저장소

- 미수집 URL 저장소는 다운로드할 URL을 보관하는 장소인데, 이 저장소를 잘 구현하면 예외를 갖춘 크롤러, URL 사이의 우선순위와 신선도를 구별하는 크롤러를 만들 수 있다고 합니다.

#### 예외

- 웹 크롤러는 짧은 시간안에 특정 서버에 많은 요청을 보낼 수 있는데, 이는 도스 공격으로 간주될 수 있습니다. 따라서 동일 웹 사이트에 대해서는 한 번에 한 페이지만 요청하고 시간차를 두고 재실행하는 로직이 갖춰있어야 합니다.

#### 우선순위

- 예를들어 Apple의 메인 페이지와 QnA 페이지의 우선순위는 유용성에 따라 우선순위가 다를것입니다. 이때 페이지랭크, 트래픽 양, 갱신 빈도와 같은 다양한 척도를 사용하여 우선순위를 결정할 수 있습니다.

#### 신선도

- 웹 페이지는 수시로 추가되고, 삭제되고, 변경됩니다. 따라서 데이터의 신선함을 유지하기 위해서는 이미 다운로드한 페이지라고 해서 주기적으로 재수집할 필요가 있습니다.
- 하지만 모든 URL을 재수집하는 것은 많은 시간과 자원이 들기 때문에 최적화 방법이 필요합니다.
  - 웹 페이지의 변경 이력을 활용합니다.
  - 우선순위를 활용하여 중요한 페이지는 더 자주 재수집합니다.

<br>

### HTML 다운로더













