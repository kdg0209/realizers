# 카프카의 기본 구성요소
<hr>

#### 주키퍼 

- 분산 애플리케이션이 안정적인 서비스를 할 수 있도록 도와주는 코디네이션 시스템입니다.
- 주키퍼가 다운되면 분산 애플리케이션 전체가 다운될 수 있으므로 안정성을 확보하기 위해 클러스터로 구축합니다.
- 주키퍼에는 카프카(브로커)의 메타 데이터나 브로커의 헬스 체크를 담당합니다.

#### 카프카(브로커)

- 메시지(레코드)를 관리하는 서버, 여러대의 브로커로 클러스터링할 수 있습니다.

#### 프로듀서

- 메시지를 발송하는 서버

#### 컨슈머

- 메시지를 소비하는 서버

#### 토픽

- 프로튜서와 컨슈머가 소통하는 창구의 역할을 수행합니다.
- 토픽의 이름은 카프카내에서 유니크해야합니다.

#### 파티션

- 하나의 토픽내에 물리적으로 나뉜 영역입니다.
- 카프카는 파티션을 통해 강력한 병렬 처리 및 고성능을 얻을 수 있습니다.

#### 세그먼트

- 프로듀서가 전송한 메시지가 브로커의 로컬 디스크에 저장되는 파일입니다.

![스크린샷 2024-01-26 오후 10 08 36](https://github.com/kdg0209/realizers/assets/80187200/d1709322-71e1-44fa-83ae-ff7eded17c74)

#### 메시지(레코드)

- 프로듀서가 브로커로 데이터를 전송하거나, 컨슈머가 읽어가는 데이터 조각입니다.

<br>

# 리플리케이션
<hr>

- 아래 명령어는 test-m-topic 토픽을 생성하며 파티션의 수는 3개, 리플리케이션은 3개를 유지하겠다는 명령어입니다.
- Isr을 보면 현재 동기화되고 있는 리플리케이션 정보를 확인할 수 있습니다.
- Replicas를 보면 현재 파티션이 복제되고 있는 브로커의 정보를 확인할 수 있습니다.
- Leader는 프로듀서, 컨슈머로부터 오는 읽기와 쓰기 요청을 처리하며, Follower는 오직 Leader로부터 리플리케이션하게 됩니다.
```
1. kafka-topics --bootstrap-server localhost:9092 --create --topic test-m-topic --partitions 3 --replication-factor 3
2. kafka-topics --bootstrap-server localhost:9092 --describe --topic test-m-topic
```
![스크린샷 2024-01-26 오후 10 58 42](https://github.com/kdg0209/realizers/assets/80187200/874291d6-fd6f-498c-904a-10250b090568)

![스크린샷 2024-01-26 오후 11 30 29](https://github.com/kdg0209/realizers/assets/80187200/48aeb0e7-6bd8-4369-83af-47762b1419ef)

<br>

#### 🤔 멀티 브로커 환경에서 replication-factor 옵션을 주지 않고 토픽을 만들면 어떻게 될까?

- Replicas, Isr의 정보를 토대로 파티션이 복제되거나, 동기화 중인 브로커가 없습니다.
```
1. kafka-topics --bootstrap-server localhost:9092 --create --topic test-2-m-topic --partitions 2
2. kafka-topics --bootstrap-server localhost:9092 --describe --topic test-2-m-topic
```

![스크린샷 2024-01-26 오후 11 16 11](https://github.com/kdg0209/realizers/assets/80187200/14b0831a-3824-4a3e-986e-d90cd754fd1d)

<br>

# 파티션
<hr>

- 하나의 토픽이 한번에 처리할 수 있는 한계를 높이기 위해 하나의 토픽을 여러개의 물리적 영역으로 나눈것을 파티션이라 합니다.
- 파티션 수는 초기 생성시 얼마든지 늘릴 수 있지만, 반대로 늘린 파티션은 다시 줄일 수 없기 때문에 신중해야 합니다.
- 처음 파티션수를 적게 가져가고, 컨슈머의 메시지 처리량을 모니터링하면서 조금씩 늘려가는 방법이 좋습니다.
  - 컨슈머의 LAG로 파악합니다.(프로듀서가 보낸 메시지 수 - 컨슈머가 가져간 메시지 수)

<br>

# 세그먼트
<hr>

- 세그먼트란 프로듀서가 전송한 메시지가 로컬 디스크에 저장되는 파일입니다. 0X20.log
- ⭐️ 참고: Key가 없는 메시지는 파티션이 여러개인 경우 순서보장을 할 수 없음. 
```
// 메시지 프로듀서 
1. kafka-console-producer --bootstrap-server localhost:9092 --topic test-m-topic --property key.separator=: --property parse.key=true

// 메시지 컨슈머
2. kafka-console-consumer --bootstrap-server localhost:9092 --topic test-m-topic --property print.key=true --property print.value=true --property print.partition=true --from-beginning

// 세그먼트 조회
3. kafka-dump-log --deep-iteration --files 00000000000000000000.log --print-data-log
```

![스크린샷 2024-01-26 오후 11 53 47](https://github.com/kdg0209/realizers/assets/80187200/2053ede0-a981-45fc-8b65-11d18b7174c1)

<br>

# 분산 시스템에서 모든 브로커 서버가 죽으면?
<hr>

- 카프카 클러스터내에 브로커 1, 2, 3이 있었는데 초록색 메시지를 Leader/Follower에게 모두 전달하였고, 5초 뒤에 브로커 3이 다운되어 카프카 클러스터내에는 브로커 1, 2만 남아있게 되었습니다.
  
![스크린샷 2024-01-27 오전 12 19 09](https://github.com/kdg0209/realizers/assets/80187200/48ee8886-639f-496b-bd3c-aec97e5f6192)

- 시간이 흘러 파란색 메시지를 Leader/Follower에게 모두 전달하였고, 3초 뒤에 브로커 2가 다운되어 카프카 클러스터내에는 브로커 1만 남아있게 되었습니다.

![스크린샷 2024-01-27 오전 12 23 04](https://github.com/kdg0209/realizers/assets/80187200/137d1b8e-5a91-400b-bac0-93e5016cf89d)

- 또 시간이 흘러 자주색 메시지를 Leader에게만 전달이 되었습니다. 그리고 1초 후에 Leader 브로커 마져 다운이 되어 카프카 클러스터내에는 실행중인 브로커가 없게되는데 이때 2가지 대응을 할 수 있습니다.

![스크린샷 2024-01-27 오전 12 25 05](https://github.com/kdg0209/realizers/assets/80187200/ddcf7849-23c4-4141-8cbf-9d8b0d776420)


### 💡 대응 방법

1. 마지막까지 Leader였던 브로커 1이 다시 실행되고, Leader가 될 때까지 기다립니다.
2. Leader/Follower 상관없이 가장 먼저 실행되는 브로커가 리더가 됩니다.
3. ⭐️ 참고: 카프카는 기본 설정으로 2번 방법을 사용합니다.

#### 1번 방법

- 1번 방법을 사용한다면 데이터 손실이 가장 적으나, 무조건 브로커 1이 먼저 실행되어야하는 조건이 있습니다. 만약 브로커 1이 결함이 생겨 바로 실행되지 않는다면 더 큰 문제로 이어질 수 있습니다.

#### 2번 방법

- 2번 방법은 가장 빨리 장애에 대응가능하지만 데이터 손실이 가장 크게 발생합니다.

<br>

# Page Cache
<hr>

### 순차 I/O

- 디스크에 순차적으로 접근하는 순차 I/O는 Random Access에 비해 150,000배 빠르다고 합니다.
- 세그먼트 파일의 자료 구조는 메시지가 중간에 삽입되지 않고, 오로지 끝에만 저장되는 append-only의 특성을 가지고 있습니다.
- 세그먼트에 쓰여진 메시지(레코드)는 수정이 불가합니다.
- 카프카는 Zero-Copy를 사용함으로써 context-switch의 발생 횟수를 줄이고, 성능을 개선하였습니다.
  - https://velog.io/@jinii/%EC%A0%9C%EB%A1%9C%EC%B9%B4%ED%94%BCzero-copy

![스크린샷 2024-01-27 오전 11 01 22](https://github.com/kdg0209/realizers/assets/80187200/e85df68d-3081-44c1-a0ef-b8a7f9f11513)

<br>

# 배치 전송 및 압축 전송
<hr>

- 프로듀서에서 send 메서드 호출 시 KafkaProducer.class의 send 메서드를 호출하게 됩니다.
- 내부에서 Serializer가 수행되어 압축이 되며, 파티셔닝이 이루어집니다.
- 그리고 RecordAccumulrator 클래스의 append 메서드가 호출되며 배치로 담기게 됩니다.
- 그리고 별개의 Sender 스레드는 RecordAccumulator에 배치로 저장된 데이터를 drain 메서드로 꺼내 kafka cluster로 데이터를 전송하는 역할을 수행합니다.(Sender.class 내부적으로 이루어짐)

![스크린샷 2024-01-27 오후 12 58 07](https://github.com/kdg0209/realizers/assets/80187200/172adbd2-4e43-43d4-a2a9-361e39941be6)


  

  


