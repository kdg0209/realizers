# 카프카 리플리케이션
<hr>

#### 토픽 생성

- 아래 명령어를 사용하여 파티션 3개, 리플리케이션 팩터 3개의 토픽을 생성합니다.
```
kafka-topics --bootstrap-server localhost:9092 --create --topic topic-p3-r3 --partitions 3 --replication-factor 3
```

#### 토픽 조회

- 토팍을 조회하면 정상적으로 만들어진 것을 알 수 있고, 파티션이 복제된것도 확인할 수 있습니다.

```
kafka-topics --bootstrap-server localhost:9092 --describe --topic topic-p3-r3
```

![스크린샷 2024-02-01 오후 10 53 29](https://github.com/kdg0209/realizers/assets/80187200/8156a4be-e70d-4c76-be93-df6e52040318)

#### 그림으로 이해하기

![스크린샷 2024-02-01 오후 10 57 52](https://github.com/kdg0209/realizers/assets/80187200/daf478b7-9bf6-4f3a-b24c-c4f0296394e4)

<br>

## Leader Partition

- Producer와 Consumer는 Leader Partition을 통해 읽기와 쓰기를 수행합니다.
- 즉 Producer는 메시지 전송시 모든 리플리케이션에 메시지를 전송하는게 아니라 Leader Partition에게만 메시지를 전송합니다. 
또한 Consumer도 Leader Partition으로부터 메시지를 가져옵니다.

#### 💡 Kafka 2.4 업데이트

- kafka 2.4 버전부터 Consumer는 Follower Partition으로부터 메시지를 가져올 수 있습니다.(Follower Fetching)
- https://www.conduktor.io/kafka/kafka-topic-replication/#acks-=-all-2
- https://developers.redhat.com/blog/2020/04/29/consuming-messages-from-closest-replicas-in-apache-kafka-2-4-0-and-amq-streams#

<br>

## Leader와 Follower의 리플리케이션 동작

- 카프카는 리플리케이션 과정에서 ACK 통신 단계를 제거함으로써 성능을 높였습니다.
- 우선 리플리케이션의 동작을 원리를 파악하기 위해서 다음과 같은 설정을 해야합니다.
- acks: all
- min.insync.replicas: 3 (최소 리플리케이션 수)

#### 과정

1. Leader Partition만이 0번 오프셋에 초록색 새로운 메시지를 가지고 있습니다.
2. Follower Partition들은 Leader에게 0번 오프셋의 메시지를 가져오기 위해 Fetch 요청을 보낸 후 새로운 메시지(초록색)가 있다는 사실을 인지하고 리플리케이션하게 됩니다.
3. Leader Partition은 모든 Follower가 0번 오프셋 메시지에 대해 Fetch 요청을 보냈다는 사실을 알고 있습니다.(하지만 Leader는 Follower가 성공적으로 리플리케이션 했는지는 모릅니다.)

![스크린샷 2024-02-01 오후 11 49 17](https://github.com/kdg0209/realizers/assets/80187200/247270aa-f086-4fb4-a7d9-a10d34db14bf)

4. 3번째 flow에서 Producer로부터 새로운 메시지를 받게됩니다.
5. 0번 오프셋에 대한 리플리케이션을 마친 Follower들은 Leader에게 다음 오프셋을 Fetch하게 됩니다.
6. Follower들로부터 1번 오프셋에 대한 Fetch 요청을 받은 Leader는 0번 오프셋이 정상적으로 복제되었음을 인지하고 오프셋 0번을 커밋하게 됩니다.
   - 만약 Follower가 0번 오프셋을 정상적으로 복제하지 못한다면 Follower는 다시 Fetch 요청을 했을 때 1번 오프셋이 아닌 0번 오프셋에 대해 Fetch 요청을 보내개 됩니다. 따라서 Leader는 Follower가 요청한 오프셋을 보고 성공했는지 인지할 수 있습니다.
7. Follower들로부터 1번 오프셋 Fetch를 받은 Leader는 응답에 0번 오프셋 메시지가 커밋되었다는 내용도 함께 전달합니다. 그리고 Leader의 응답을 받은 모든 Follower도 0번 오프셋 메시지가 커밋되었다는 것을 알게되고, 커밋을 표시하게 됩니다.

![스크린샷 2024-02-01 오후 11 56 28](https://github.com/kdg0209/realizers/assets/80187200/c5a4102e-4276-4100-baf6-6215d539c937)

<br>

## Isr(In-Sync-Replica)

- Leader와 Follower는 Isr이라는 논리적 그룹으로 묶여 있습니다. 
- 논리적 그룹으로 나누는 이유는 해당 그룹에 속한 Follower들만이 Leader가 죽은 경우 새로운 Leader의 자격을 가질 수 있기 때문입니다. 다시말해 Isr 그룹에 속하지 못한 Follower는 새로운 Reader의 자격이 될 수 없습니다.
- 위 리플리케이션 동작 과정을 통해 Isr내에 있는 모든 Follower에게 복제가 완료되면, Leader는 내부적으로 커밋되었다는 표시를 하게된다는 것을 알게되었습니다. 마지막 오프셋 위치는 하이워터마크라고 부릅니다.
- 즉 커밋되었다는 것은 리플리케이션 팩터 수의 모든 리플리케이션이 전부 메시지를 저장했음을 의미합니다. 그리고 이렇게 커밋된 메시지만 Consumer가 읽어갈 수 있습니다.

<br>

## Leader Epoch와 복구

<br>

## 로그(세그먼트)

- 카프카의 토픽으로 들어오는 레코드(메시지)는 segment라는 파일에 저장됩니다.
- segment에는 key, value, offset, payload와 같은 정보가 브로커의 로컬 디스크에 저장됩니다.
- segment의 기본 크기는 1GB로 설정되어 있습니다. segment의 크기가 1GB보다 커지는 경우 해당 segment는 close되고, 롤링 전략을 적용되게 됩니다.
- close된 segment는 읽기만 가능합니다.

<img width="960" alt="스크린샷 2024-02-03 오후 1 13 28" src="https://github.com/kdg0209/realizers/assets/80187200/47c9a4fb-de48-416b-9af2-62cd6201cb32">

<br>


### rolling 매커니즘 테스트

#### segment.bytes를 10KB로 하여 테스트 진행

```
1. 예제 토픽 생성
kafka-topics --bootstrap-server localhost:9092 --create --topic segment-test-topic --partitions 3

2. 생성한 토픽의 segment byte 수정
kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name segment-test-topic --alter --add-config segment.bytes=10240

3. 변경한 내용 확인
kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name segment-test-topic --all --describe | grep segment.bytes

4. 아래와 같이 출력되면 정상적으로 변경된 것
segment.bytes=10240 sensitive=false synonyms={DYNAMIC_TOPIC_CONFIG:segment.bytes=10240, STATIC_BROKER_CONFIG:log.segment.bytes=1073741824, DEFAULT_CONFIG:log.segment.bytes=1073741824}

5. 테스트 메시지 파일 작성
for i in {1..2000}
do
echo "test message send test-i" >> message.log
done

6. 파일 기반으로 메시지 전송
kafka-console-producer --bootstrap-server localhost:9092 --topic segment-test-topic < message.log
```

<img width="1038" alt="스크린샷 2024-02-03 오후 1 46 50" src="https://github.com/kdg0209/realizers/assets/80187200/5cbdbfac-0c84-4888-b5a3-d3cba6a69c2b">

#### segment.ms를 60초 하여 테스트 진행

```
1. segment.byte를 기본 속성값으로 되돌림
kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name segment-test-topic --alter --delete-config segment.bytes

2. segment.ms를 60초로 수정
kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name segment-test-topic --alter --add-config segment.ms=60000

3. 파일 기반으로 메시지 전송
kafka-console-producer --bootstrap-server localhost:9092 --topic segment-test-topic < message.log
```

<img width="997" alt="스크린샷 2024-02-03 오후 2 02 38" src="https://github.com/kdg0209/realizers/assets/80187200/2ed997f7-fd56-46ec-b186-3268ad2fb62a">


#### 🤔 index, timeindex는 무엇일까?

- 우선 log.index.interval.bytes를 알아야 하는데 해당 속성은 새로운 항목이 offset index에 추가되는 주기이다.
- index는 모든 offset에 대한 byte position을 가지고 있지 않으며, log.index.interval.bytes에 설정된 만큼의 segment byte가 만들어질때마다 해당 offeset에 대한 byte position 정보를 가지고 있습니다.
- timeindex는 메시지의 생성 Unix 시간을 ms 단위로 가지고 있고, 해당 생성 시간에 해당하는 offset정보를 가지고 있습니다.
- 따라서 index, timeindex를 가지고 random access시 파일의 용량만큼 수행 시간(scan 시간)이 소요되는게 아니라 조금 더 효율적으로 찾을 수 있게 됩니다.

```
1. log.index.interval.bytes 확인 (기본 4KB)
kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name segment-test-topic --all --describe | grep log.index.interval.bytes
```

<img width="1041" alt="스크린샷 2024-02-03 오후 2 31 49" src="https://github.com/kdg0209/realizers/assets/80187200/35b0cd81-5a85-47a9-ae0f-9aa953945285">

<br>

### Log Cleanup Policy 

#### delete

- delete로 설정하면 segment를 log.retention.hours나 log.retention.bytes 설정값에 따라 삭제합니다.

#### compact

- compact로 설정하면 segment를 key 값 레벨로 가장 최신의 메시지만 유지하도록 segment를 재구성 합니다.

#### delete, compact

- compact와 delete를 함께 적용합니다.

#### 설정 값

- log.retention.hours: 개별 segment의 유지 시간(기본 7일)
- log.retention.bytes: 삭제하기 전에 보관할 파티션 당 로그 수. 기본값은 -1. 토픽별로 설정이 가능하며, 로그의 시간이나 크기제한에 도달하면 삭제됩니다.
- log.retention.check.interval.ms: 브로커가 백그라운드로 삭제할 segment를 찾는 주기입니다.

#### delete

```
1. 기본 삭제 정책 조회(기본: delete)
kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name segment-test-topic --all --describe | grep log.cleanup.policy

2. 브로커의 segment 유지 시간 조회(기본: 7일)
kafka-configs --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --all --describe | grep log.retention.hours

3. 토픽의 retention 확인(기본: retention.ms=604800000(7일))
kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name segment-test-topic --all --describe | grep retention

4. 테스트를 위해 retention.bytes를 10KB로 수정
kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name segment-test-topic --alter --add-config retention.bytes=10240

5. 테스트를 위해 retention.ms를 3분으로 수정
kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name segment-test-topic --alter --add-config retention.ms=180000
```

#### 💡 참고

- Active Segment지만 아무런 메시지가 안온 상태에서 log.retention.hours의 시간이 지나면 삭제 대상이 될 수 있습니다.

<img width="1026" alt="스크린샷 2024-02-03 오후 3 05 52" src="https://github.com/kdg0209/realizers/assets/80187200/7670d3bc-9c68-49cf-b439-b823ededc60c">

#### compact















