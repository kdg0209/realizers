# 트랜스포머 아키텍처

- 트랜스포머 아키텍처는 언어를 이해하는 `인코더`와 언어를 생성하는 `디코더` 부분으로 나눌 수 있습니다.

<br>

## 1. RNN?

#### 💡 RNN이란?

- `RNN`은 과거의 정보를 현재의 입력과 함께 처리하는 방식입니다.
- 즉, 입력하는 텍스트를 `순차적`으로 처리하며 다음 단어를 예측하게 됩니다.
- `RNN`에는 `은닉 상태`라는 개념이 있는데, `은닉 상태`는 이전 입력에 대한 정보를 압축하여 다음으로 넘겨주는 역할을 수행하게 됩니다.
- 아래 그림에서 X는 텍스트의 토큰을 의미합니다. H는 입력 토큰을 다시 RNN 모델에 입력했을 때의 출력인데, 그림에서 확인할 수 있듯이 이전 토큰의 출력(`은닉 상태`)을 다시 모델의 입력으로 사용하기 때문에 병렬처리에는 적합하지 않습니다.

<img width="1032" alt="스크린샷 2025-05-24 오후 2 56 13" src="https://github.com/user-attachments/assets/3d926da6-30fb-44ef-8aa3-762185845a84" />

<br><br>

## 2. 트랜스포머 아키텍처란?

- 트랜스포머는 RNN의 문제를 해결하기 위해 순차처리하는 방식을 버리고 `셀프 어텐션`이라는 개념을 도입하였습니다.
- `셀프 어텐션`이란 입력된 문장 내의 각 단어가 서로 어떤 관련이 있는지 계산해서 각 단어의 표현을 조정하는 역할을 수행합니다.
- 트랜스포머 아키텍처는 언어를 이해하는 `인코더`와 언어를 생성하는 `디코더`로 나뉘고, 아래 과정을 거치게 됩니다.
  - 인코더에서는 입력 받은 문자열을 토큰 임베딩을 통해 변환하고 위치 인코딩에서 문장의 위치 정보를 더하게 됩니다. 그리고 층 정규화, 멀티 헤드 어텐션, 피드 포워드 층을 거치게 됩니다.
  - 디코더에서는 인코더의 결과를 토대로 토큰 임베딩을 통해 변환하고 위치 인코딩에서 문장의 위치 저보를 더하고 층 정규화, 마스크 멀티 헤드 어텐션, 크로스 어텐션, 피드 포워드 층을 거치게 됩니다.

<img width="1032" alt="스크린샷 2025-05-24 오후 3 23 38" src="https://github.com/user-attachments/assets/9a4e34e2-64ef-4cf1-bd2e-04c7cbada7bf" />

<br><br>

### 2-1. 텍스트를 임베딩으로 변환하기

- 텍스트를 적절한 단위로 잘라 숫자형 아이디를 부여하는 `토큰화`를 수행하며, 디음으로 토큰 아이디를 토큰 임베딩 층을 통해 여러 숫자의 집합인 토큰 임베딩으로 변환합니다. 마지막으로 위치 인코딩 층을 통해 토큰의 위치 정보를 담고 있는 위치 임베딩을 추가해 최종적으로 모델에 입력할 임베딩을 만들게 됩니다.

#### 토큰화

- 토큰화란 텍스트를 적절한 단위로 나누고 숫자 아이디를 부여하는 것을 의미합니다.
- 한글은 자음과 모음 단위부터 크게는 단어 단위로 나눌 수 있습니다. 이때 음절은 중간 정도 단위로 볼 수 있습니다.
- 단어 단위를 기준으로 토큰화할수록 텍스트 의미가 잘 유지되지만 사전 크기가 커진다는 단점이 있고, 이전에 본 적이 없는 새로운 단어는 사전에 없기 때문에 처리하지 못하는 `OOV` 문제가 발생하게 됩니다. 반대로 작은 단위로 토큰화할수록 `OOV` 문제는 줄일 수 있지만 텍스트의 의미가 유지되지 않게됩니다.
  
<img width="1032" alt="스크린샷 2025-05-24 오후 3 56 32" src="https://github.com/user-attachments/assets/95af11f3-116b-408a-a88b-5212e1ccc616" />

<br>

#### 서브워드 토큰화

- 작은 단위로 나누면 OOV을 줄일 수 있고, 큰 단위로 나누면 텍스트의 의미를 잘 유지할 수 있는데, 이러한 장점만 살린게 `서브워드 토큰화`라고 합니다.
- `서브워드 토큰화`는 같은 데이터에 등장하는 빈도에 따라 토큰화 단위를 결정하는 방식입니다.
- 즉, 자주 나오는 단어는 단어 단위 그대로 유지하고 가끔 나오는 단어는 더 작은 단위로 나눠 텍스트의 의미를 최대한 유지하면서 사전의 크기는 작고 효율적으로 유지할 수 있습니다.

#### 위치 인코딩

- 트랜스포머는 사용자의 입력을 병렬적으로 처리하는데, 그 과정에서 순서 정보가 사라지게 됩니다. 텍스트에서 순서는 매우 중요한 정보를 담당하기 때문에 어떻게든 순서 정보를 알아야 하는데, 이를 `위치 인코딩`이 담당해줍니다.
- 예를들어 "나는 밥을 먹었다"와 "먹었다 밥을 나는"을 구분할 수 없기 때문에 단어 순서 정보를 임의적으로 추가해줍니다.

<br>

### 2-2. 어텐션이란 무엇일까?

- 어텐션의 사전적 의미는 `주의`인데, 이는 텍스트를 처리하는 관점에서 입력한 텍스트에서 어떤 단어가 서로 관련되는지 `주의를 기울여` 파악한다라는 의미로 이해될 수 있습니다.

#### 사람이 글을 읽는 방벅과 어텐션

- 사람이 글을 읽을 떼 왼쪽부터 오른쪽 순서로 글을 읽으며, 복잡하고 어려운 글을 읽을 때는 주변 맥락을 통해 심층적으로 이해하기도 합니다. 즉, 여기서 `어텐션`의 아이디어를 엿볼 수 있는데 사람이 단어 사이의 관계를 고민하는 과정(맥락을 추론하는 과정)을 딥러닝 모델이 수행할 수 있도록 모방한 것입니다.
- 아래 첫 번째 문맥에서 파리라는 단어는 곤충을 의미하는지 도시를 의미하는지 파악하기 힘듭니다. 하지만 두 번째 문맥에서는 파리가 도시임을 정확히 알 수 있습니다. 즉 맥락을 통해 알 수 있었습니다.
- 사람은 자연스럽게 관련 있는 단어를 찾아 그 맥락을 반영해 단어를 재해석할 수 있지만 컴퓨터는 재해석할 수 없는데 어떻게 해야할까요?
  - 단어와 단어 사이 관계를 계산해서 그 값에 따라 관련이 깊은 단어와 그렇지 않은 단어를 구분할 수 있어야 합니다.
  - 관련이 깊은 단어는 더 많이 맥락을 반영하고 그렇지 않은 단어는 더 적게 반영해야 합니다.

> 1. OO OO 파리 OOO OOOO
> 2. 나는 최근 파리 여행을 다녀왔다

<br>

#### 쿼리, 키, 값 이해하기

- 단어와 단어 사이 관계를 계산해야 하는데, `쿼리`, `키`, `값`이라는 개념이 활용됩니다.

> 쿼리: 검색창에 검색할 때 우리가 입력하는 검색어가 쿼리(query)에 해당됩니다. <br>
> 키: 쿼리와 관련 있는지 계산하기 위해 문서가 가진 특징을 키라 합니다. 예를들어 문서의 제목, 저자, 본문등이 활용될 수 있습니다. <br>
> 값: 제공할 데이터 자체를 의미합니다.

<img width="1032" alt="스크린샷 2025-05-25 오후 1 24 30" src="https://github.com/user-attachments/assets/72e47815-4f94-4a5e-9a35-e5a2eeba65d2" />

<br><br>

🔥 모방의 첫 번째: `평균`

- 평균은 모든 단어(토큰)의 값을 동일한 중요도로 평균을 내서 문맥 정보를 구성하는 방식입니다.
- 위 그림을 토대로 접근을 해보면 아래와 같습니다. 이러한 방법은 결국 `가중치`를 고려할 수 없게 됩니다.

> 최종 출력 = (값₁ + 값₂ + 값₃ + ... + 값₅) / 5

<br>

🔥 모방의 두번째: `가중치`

- 가중치는 쿼리(query)와 키(key) 간의 유사도를 계산해 어떤 값에 얼마나 `주의(attend)`할지 결정하는 수치입니다.
  - 중요한 단어에는 높은 가중치, 덜 중요한 단어에는 낮은 가중치!

계산 방식

1. 쿼리, 키, 값을 통해 각각 벡터를 생성합니다.
    - 각 단어를 세 개의 벡터로 변환(Q, K, V)
2. 쿼리와 키의 내적 계산
    - Q와 모든 K 간 유사도 계산
3. 스케일링 + 소프트맥스를 적용합니다.
    - 확률처럼 정규화된 가중치
4. 가중치를 값에 곱하고 합산하여 결과를 도출합니다.
    - 최종 출력

<br>

#### 멀티 헤드 어텐션 이해하기

- `멀티 헤드 어텐셔`이란 한 번에 하나의 어텐션만 수행하는게 아니라 여러 어텐션 연산을 동시에 적용하는 것입니다.
- 토큰 사이의 관계를 한 가지 측면에서 이해하는 것보다 여러 측면을 동시에 고려할 때 언어나 문장에 대한 이해도가 높아질 것이다라고 생각을 한 것입니다.

<br>

### 2-3. 정규화와 피드 포워드 층

#### 정규화란?

- `정규화`란 딥러닝 모델에서 입력이 일정한 분포를 갖도록 만들어 학습이 안정적이고 빨라질 수 있도록 하는 기법입니다.
- 딥러닝 모델에 데이터를 입력할 때, 입력 데이터의 분포가 서로 다르면 모델 학습이 잘되지 않기 때문에 데이터를 정규화하는게 중요합니다. 예를들어 사람의 나이와 키를 통해 몸무게를 예측하는 모델을 만들 때, 나이는 보통 1~100세 사이의 값을 가지며, 키는 140CM ~ 200CM 사이의 값을 가진다고 가정할 때 만약 키 데이터를 CM이 아닌 mm 단위를 사용한다면 분포가 더 넓어지게 됩니다. 그럼 모델이 키 데이터의 중요성을 과대평가할 가능성이 높아지고 정확한 예측을 어렵게 만듭니다.
- 위와 같은 문제를 해결하기 위해 데이터를 `정규화`하여 모든 입력 변수가 비슷한 범위와 분포를 갖도록 조정하는 것입니다.

🔥 정규화는 어떻게 구하는가?

- 여러 데이터의 평균과 표준편차를 구해서 다음과 같은 식으로 계산한다고 합니다.
- 벡터 x를 정규화한 norm_x는 벡터 x에서 x의 평균을 빼고 x의 평균편차로 나눠 평균이 0이고 표준편차가 1인 분포

> norm_x = (x - 평균) / 표준편차

#### 📌 배치 정규화?

- 이미지 처리에서는 보통 배치 정규화를 사용한다고 합니다.
- 배치 정규화는 아래 그림에서 볼 수 있듯이 입력으로 들어가는 미니 배치 사이에 정규화를 수행합니다.

<img width="1032" alt="스크린샷 2025-05-25 오후 2 55 25" src="https://github.com/user-attachments/assets/614962b7-aecd-4cbb-b9a8-c3a8064b8697" />

<br><br>

#### 📌 층 정규화?

- 층 정규화는 배치 정규화의 단점을 보완하는데, 각 토큰 임베딩의 평균과 표준편차를 구해 정규화를 수행합니다.
- 자연어 처리에서는 입력으로 들어가는 문장의 길이가 다양하기 때문에 배치 정규화를 사용하면 정규화 효과를 보장하기 어렵다고 합니다.
- 문장별로 실제 데이터의 수가 다르더라도 각각의 토큰 임베딩별로 정규화를 수행합니다.

<img width="1032" alt="스크린샷 2025-05-25 오후 3 04 05" src="https://github.com/user-attachments/assets/e62f5b5a-0a06-4a86-bb33-9744089b5c14" />

<br><br>

#### 피드 포워드 층이란?

- `피드 포워드 층`은 데이터의 특징을 학습하는 완전 연결 층을 의미합니다.
- `멀티 헤드 어텐션`이 단어 사이의 관계를 파악하는 역할이라면 `피드 포워드 층`은 입력 텍스트 전체를 이해하는 역할을 담당합니다.

<br>

### 2-4. 인코더

- 트랜스포머 인코더는 `멀티 헤드 어텐션`, `층 정규화`, `피드 포워드 층`이 반복되는 형태입니다.
- 또한, 상단의 그림을 다시 보면 Nx가 정의되어 있는데, 이는 Nx번 반복되어 입력을 다시 더해주는 형태가 됩니다. (`잔차 연결`)

<br>

### 2-5. 디코더

- 디코더는 생성을 담당하는 부분으로, 사람이 글을 쓸 때 앞 단어부터 순차적으로 작성하는 것처럼 트랜스포머 모델도 앞에서 생성한 토큰을 기반으로 다음 토큰을 생성합니다. 이러한 특징을 `인과적` 또는 `자기 회귀적`이라 합니다.
- 디코더는 인코더와 다르게 `마스크 멀티 헤드 어텐션`과 `크로스 어텐션`을 사용합니다.

<br>

#### 참고

- https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding



