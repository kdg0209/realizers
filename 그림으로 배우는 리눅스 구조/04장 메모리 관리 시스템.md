# 메모리 관리 시스템

- 리눅스는 시스템에 설치된 메모리 전체를 `메모리 관리 시스템` 기능으로 메모리를 관리합니다.

<br>

## 1. 메모리 관련 정보 수집히기

### 1-1. free

- 리눅스에서 `free`명령어를 통해 설치된 메모리 용량과 사용중인 메모리를 파악할 수 있습니다.
- `free` 명령어는 기본적으로 킬로바이트로 표현되므로 `-h` 옵션을 사용하여 MB/GB 단위로 변환

```text
> free -h
               total        used        free      shared  buff/cache   available
Mem:           7.7Gi       494Mi       6.0Gi       301Mi       1.2Gi       6.7Gi
Swap:          1.0Gi          0B       1.0Gi
```

<br>

#### total

- 시스템에 설치된 전체 메모리 용량을 의미합니다.
- 현재 사용중인 리눅스의 전체 메모리 용량은 7.7G를 의미합니다.

<br>

#### used

- `used`는 시스템이 사용중인 메모리 용량을 의미합니다.
- used = total - free - buff/cache의 값으로 계산됩니다. (커널이 캐시로 점유중인 메모리 값도 포함해야 합니다.)
- 실제로 사용 가능한 메모리 공간을 보려고하면 `available` 값을 참고하는게 좋습니다.

<br>

#### free

- `free`는 시스템에서 정말 비어있는 메모리 공간을 의미합니다.
- 하지만 리눅스는 사용하지 않는 메모리 공간은 낭비라고 생각하기 때문에 `buffer/cache` 공간으로 적극활용합니다.

<br>

#### shared

- `shared`는 여러 프로세스가 공유하고 있는 메모리 공간을 의미합니다.
- `shared`는 `shared memory`영역을 의미하며, 프로세스 간 통신(`IPC`)용으로 사용되는 메모리 등이 있습니다.

<br>

#### buff/cache

- `buff/cache`는 버퍼 캐시 + 페이지 캐시를 합한 값입니다.
- 리눅스가 성능 최적화를 위해 디스크에 적재되어 있는 데이터를 메모리에 캐싱해두는 것을 의미합니다.

💡 buffer cache

- 블록 디바이스에 대한 메타데이터를 메모리에 저장시킵니다. 블록 디바이스란 CD/DVD, 하드 디스크 등의 저장장치를 말합니다.
- 즉 블록 디바이스에 대한 파일 이름, 마지막 수정 날짜. 파일 형식 등을 저장합니다.
- 쓰기 작업 시에 활용됩니다.
  - 보통 쓰기 시 작업은 디스크에 바로 하지 않고, 메모리에 임시 저장한 뒤 나중에 한번에 처리됩니다.
  - 디스크 I/O는 상대적으로 느리기 때문에 우선 쓰기 요청을 메모리(`buffer cache`)에 저장한 후 한번에 처리되게 됩니다. (`write-back` 방식)
  - buffer cache에 저장된 데이터는 디스크와 동기화되지 않은 상태이기 때문에 `dirty page`로 분류되고 나중에 flush하여 디스크에 저장됩니다.

💡 page cache

- 저장 장치를 통해 읽어온 파일의 내용을 메모미에 저장합니다.
- `buffer cache`와 다른점은 buffer cache는 메타데이터 정보를 메모리에 저장시키는 것이지만 `page cache`는 파일의 내용을 저장한다는 것입니다.
- 읽기 작업 시 속도 향상을 위해 사용됩니다.

#### 🧐 Buffer Cache랑 Page Cache는 언제 비워질까?

- 메모리가 부족하거나, 시스템 종료 및 부팅 시, 사용자가 임의로 작업 시

<br>

#### available

- `available`는 새로운 프로세스가 메모리를 요청했을 때 문제없이 할당해줄 수 있는 메모리 공간을 의미합니다.
- available = free + (일부 page cache) + (일부 slab reclaimable)

#### 🧐 available != free + buffer/cache ??

- 처음에는 available를 계산할 때 free + buffer/cache인 줄 알았지만 `buffer/cache`는 모두 회수 가능한게 아니라고 합니다. `buffer/cache`는 캐시용 메모리 영역이긴 하지만 이 중에서 절대 회수해서는 안되는 부분이 있습니다.(`dirty page`) 그렇기 때문에 일부 영역이 포함되고, `slab`영역도 포함이 된다고 합니다.

<br>

## 2. 메모리 재활용 처리

### 2-1. 프로세스 삭제와 메모리 강제 해제

- 재활용 가능한 메모리를 해제해도 사용가능한 메모리가 없으면 `OOM`이 발생하게 됩니다. 이때 메모리 관리 시스템이 적당한 프로세스를 골라 강제 종료 시키고 유휴 메모리 공간을 만드는 `OOM Killer`라는 기능이 동작하게 된다고 합니다.

#### OOM Killer 

- OOM Killer가 동작하는 시스템이라면 메모리가 충분하지 않기 때문에 동시에 실행중인 프로세스 개수를 줄이거나 메모리를 추가적으로 늘려야 합니다.
- 또한 `메모리 누수`가 발생하고 있는지 여부도 파악해야 합니다.

<br>

## 3. 가상 메모리

### 3-1. 가상 메모리가 없었던 시절엔 무슨 문제가 있었을까?

#### 메모리 단편화(외부 단편화)

- 프로세스를 생성하고 메모리를 확보하고, 해제하는 작업에서 `메모리 단편화` 문제가 발생합니다. 단편화에는 `외부 단편화`와 `내부 단편화`로 구분할 수 있으며, 여기서는 `외부 단편화`문제가 발생합니다.
- 프로세스 사이사이에 사용하지 못하는 메모리 블록이 있습니다. 아래 그림을 보면 C 프로세스가 필요로 하는 메모리 공간은 80MB인데, 사용 가능한 메모리 공간은 50MB인 두 공간이 있습니다.

#### 🤔 압축(Compaction)을 통해서할 수 없을까?

- 우선 압축이란 사용중인 메모리 블록을 한쪽으로 몰아서 다른 한쪽에 사용 가능한 메모리 공간을 확보하는 것을 의미합니다.
- 그 시절에는 재배치 가능 코드(`elocatable code`)가 없었습니다... 그 시절에는 대부분 절대 주소를 기반으로 작성되었기 때문에 압축으로 해결할 수 없었습니다.

<img width="1032" alt="스크린샷 2025-04-19 오후 3 38 16" src="https://github.com/user-attachments/assets/081387ad-d564-4356-be5f-4782fe1f81ad" />

<br><br>

#### 멀티 프로세스 구현이 어려움

- 프로세스 A,B가 동시에 메모리에 적재된다고 가정했을 때 둘 다 시작주소를 0x00000000으로 하면 충돌이 발생하게 됩니다.

#### 메모리 보호가 없음

- 프로세스 A가 0x1000 ~ 0x1FFF까지 메모리 사용 중
- 프로세스 B가 0x2000 ~ 0x2FFF까지 메모리 사용 중
- 가상 메모리에서는 프로세스 B가 프로세스 A의 메모리 공간을 침범할 수 없었지만 절대 주소를 사용하던 시절에는 접근이 가능했다고 합니다.

#### 컨텍스트 스위칭의 어려움

- 가상 메모리가 있는 시절에는 MMU가 페이지 테이블을 이용해 쉽게 전환이 가능했지만 가상 메모리가 없던 시절에는 A가 사용하던 물리 메모리는 B가 사용하면 안되니 완전히 정리를 해야하고, B의 물리 메모리 공간도 확보해야하며 이러한 작업이 반복적으로 이루어졌다고 합니다.

<br>

### 3-2. 가상 메모리의 등장

#### 💡 가상 메모리란?

- 가상 메모리는 프로세스가 메모리에 접근할 때 직접적으로 접근하는게 아니라 `가상 주소`를 사용하여 간접적으로 접근하는 기능입니다.

#### 우리는 벌써 가상 메모리를 사용하고 있다

- 아래 자바 코드가 있다고 생각해봅시다.
- 그럼 JVM은 힙 영역에 o를 저장합니다.
- o가 할당된 메모리 주소는 0x7ffd1234이며, 이 주소는 가상 주소입니다.
- CPU는 0x7ffd1234 -> MMU -> 페이지 테이블 -> 물리주소로 변환되게 됩니다. 그리고 이 주소에는 o의 값이 저장되어 있겠죠?

```java
Object o = new Object();

┌────────────┐
│  변수 o     │──▶ [가상 주소: 0x7ffd1234]
└────────────┘              │
                            ▼
                   [MMU: 페이지 테이블 조회]
                            ▼
                   [물리 주소: 0x1abc5678]
                            ▼
                   [RAM 실제 메모리 공간]
```

<br>

#### 💡 MMU란?

- MMU(`Memory Management Unit`)는 CPU 내부에 있는 하드웨어 장치로 가상 주소를 물리 주소로 변환시켜주는 장치입니다.
- MMU를 통해 가상 주소를 물리로 변환할 뿐만 아니라 다른 프로세스가 나의 메모리 공간에 침법할 수 없도록 메모리를 보호하고 페이지 교체 감지를 할 수 있도록 합니다.
  - 가상 주소를 MMU를 통해 물리 주소를 변환하는데 이때 페이지 테이블을 참고합니다. 페이지 테이블에서 매핑되는 정보가 없으면 페이지 폴트가 발생합니다.
 
```txt
가상 주소 0x7fff0000 ──▶ MMU ──▶ 페이지 테이블 확인
                                │
                          [❌ 매핑 없음!]
                                ▼
                         페이지 폴트 발생
                                ▼
                     OS가 디스크에서 데이터 로드
                                ▼
                    물리 메모리 0x12345000에 저장
                                ▼
           페이지 테이블에 매핑 등록: 0x7fff0000 → 0x12345000
                                ▼
                    다시 프로그램 실행 → 정상 접근!
```

<br>

#### 💡 페이지 테이블이란?

- 가상 주소를 물리 주소로 바꾸기 위한 매핑 정보를 담고 있는 테이블입니다.
- CPU는 모든 메모리를 작은 블록(4KB)이라 불리는 페이지로 나눠서 관리하고, 가상 주소는 페이지 번호 + 오프셋으로 나뉘고 MMU가 이걸 이용해 물리 주소로 변환합니다.
- 페이지 테이블에서 한 페이지에 대응하는 데이터를 `페이지 테이블 엔트리`라고 부립니다. `페이지 테이블 엔트리`는 가상 주소와 물리 주소 대응 정보를 포함합니다.

🚗 흐름

- 가상 주소 = 페이지 번호 + 오프셋
  - 페이지 번호를 사용해 어떤 페이지인지 확인
  - 오프셋을 사용해 그 페이지안에서 시작 번지로부터 얼마만큼 떨여져 있는지 알기 위한 정보

📌 예시

- 가상 주소: 0x7fff1234
- 페이지 번호: 0x7fff1
- 오프셋: 0x234

```txt
[가상 주소: 0x7fff1234]
        │
        ▼
 ┌───────────────┐
 │ 페이지 번호      │ 0x7fff1
 └───────────────┘
        │
        ▼
 MMU → 페이지 테이블 → 물리 프레임 0x1234a000
        │
        ▼
 오프셋 0x234 더해서
 → 최종 물리 주소: 0x1234a234
``` 

<br>

#### 🤔 페이지 테이블의 단점이 뭘까?

- 페이지 테이블을 사용할 경우 가상 주소를 물리 주소로 변환하기 위해 먼저 페이지 테이블에 한 번 접근하고, 이 후 해당 물리 주소에 한 번더 접근해야 하므로 총 2번의 메모리 접근이 발생하는 오버헤드가 존재합니다.

<img width="1032" alt="스크린샷 2025-04-19 오후 4 46 08" src="https://github.com/user-attachments/assets/650ab4bc-2b29-4be6-914d-9bb78816acfe" />

<br><br>

#### TLB

- 페이지 테이블을 사용하는 경우 메모리에 2번 접근하는 오버헤드가 발생하게 되는데, `TLB`라는 특수한 하드웨어 캐시를 사용하여 이 문제를 해결할 수 있습니다.
- `TLB`는 캐시 메모리로 `참조 지역성의 원리`를 활용하여 페이지 테이블에서 빈번히 참조되는 일부 엔트리를 캐싱하고 있습니다.
- CPU는 페이지 테이블보다 `TLB`를 우선적으로 참조하여 원하는 페이지 번호가 `TLB`에 있는 경우 `TLB 히트`가 되어 바로 프레임 번호를 알 수 있지만 그렇지 않은 경우에는 `TLB 미스`가 발생하여 메인 메모리에 있는 페이지 테이블을 참조하게 됩니다.

<img width="1032" alt="스크린샷 2025-04-19 오후 4 54 55" src="https://github.com/user-attachments/assets/3da3370b-27a3-4251-ad8f-0a01b88dc214" />

<br><br>

#### 페이지 폴트란?

- 가상 주소를 통해서 실제 물리 주소를 얻고 메모리에 접근했지만 내가 원하는 페이지가 실제 메모리에 적재되어 있지 않을수도 있습니다. 즉, 내가 원하는 페이지가 실제 메모리에 적재되어 있는 않을 때를 `페이지 폴트`라 합니다.
- 페이지 폴트가 발생하면 CPU에서 실행 중인 명령이 중단되고, 커널 메모리에 배치된 `페이지 폴트 핸들러`처리가 실행됩니다.
- 커널은 `페이지 폴트 핸들러`를 이용하게 되는데 정상적인 요청이라면 보조기억장치에서 지금 내가 필요한 페이지를 가져오게 됩니다. 만약 비정상적인 요청이라면 `SIGSEGV` 시그널을 프로세스에게 전달하고 `SIGSEGV` 시그널을 받은 프로세스는 보통 강제종료되게 됩니다.

<br>

### 3-3. 가상 메모리의 문제 해결

#### 메모리 단편화(외부 단편화)

- 가상 메모리는 메모리를 작은 블록(4KB)이라 불리는 작은 페이지로 나눠서 관리하고, 이를 비연속적인 물리 프레임에 매핑함으로써 외부 단편화의 문제를 해결할 수 있습니다.

📌 내부 단편화는 해결하지 못함

- 메모리를 작은 블록(4KB)으로 제공하고 있지만 실제로 3.3KB만 사용하면 나머지 0.7KB는 낭비되는 메모리 공간이 됩니다.
- 이러한 문제를 해결하기 위해서 `슬랩 할당자(Slab Allocator)`, `메모리 풀`

#### 💡 슬랩 할당자(Slab Allocator)란?

- 리눅스 커널은 메모리를 기본적으로 페이지(4KB) 단위로 관리하는데, 만약 커널 내부에서 자주 쓰이는 16byte 구조체를 할당받는다면 매번 4KB를 통째로 쓰게 되어 심각한 `내부 단편화`가 발생하게 됩니다.
- 이를 해결하기 위해 `슬랩 할당자(Slab Allocator)`는 페이지 하나를 미리 여러 개의 작은 조각(슬롯)으로 쪼개두고 요청이 오면 그 중 하나를 반환함으로써 메모리를 효율적으로 활용합니다.
- `슬랩 할당자`는 자주 사용하는 크기의 객체들을 위한 풀(`캐시`)을 미리 만들어 두고 메모리 요청이 오면 해당 크기에 맞는 영역을 풀(`캐시`)에서 미리 분할해 놓은 슬롯을 꺼내 제공합니다.
- `슬랩 할당자`는 커널에서 관리하는 동적 메모리 할당자입니다. 메모리 풀 구조를 가지고 있기 때문에 미리 고정된 크기의 메모리 블록을 할당해 놓습니다.

```txt
슬랩 캐시 (Slab Cache)
   └ 슬랩 (Slab) – 페이지 단위 블록
        └ 슬롯 (Object) – 실제 반환되는 고정 크기 메모리 블록
```

<br>

#### 멀티 프로세스 구현이 어려움

- 가상 메모리는 프로세스마다 완전히 독립적인 주소 공간을 제공합니다. 따라서 서로 다른 프로세스가 같은 주소를 사용하더라도 충돌없이 독립적으로 실행할 수 있습니다.
- 프로세스 A,B는 같은 가상 주소(0x00000000)를 사용하더라고 `페이지 테이블` 덕분에 각자 다른 물리 주소로 매핑됩니다. 이렇게 되면 컨텍스트 스위칭시에도 `페이지 테이블`의 포인터만 교체하면 되므로 물리 메모리를 재배치하거나 초기화할 필요가 없어 비용이 낮아지게 됩니다.

```txt
[프로세스 A]
0x00000000 → 나만의 코드
0x00100000 → 나만의 데이터

[프로세스 B]
0x00000000 → 또 나만의 코드
0x00100000 → 또 나만의 데이터
```

<br>

## 4. 프로세스에 새로운 메모리 할당하기

- 커널이 프로세스에 메모리를 할당하는 과정은 다음과 같다고 합니다.
  - 프로세스는 xxx바이트가 메모리가 필요하다고 system call을 통해 커널에게 요청합니다.
  - 커널은 시스템의 비어있는 메모리에서 xxx바이트 영역을 확보합니다.
  - 확보한 메모리 영역을 프로세스의 가상 주소 공간과 매핑합니다.
  - 가상 주소 공간의 시작 주소를 프로세스에게 반환해줍니다.
 
- 하지만 메모리는 확보한 순간에 당장 사용하기보다는 조금 시간이 지난 후에 사용하는 일이 많아 리눅스는 메모리를 확보하는 과정을 두 단계로 나눴다고 합니다.
  - 메모리 영역 할당: 가상 주소 공간에 자리를 미리 찜해두는 것입니다. 0x00000000 ~ 0x00010000까지는 A라는 프로세스가 사용할거니까 미리 확보해놔야지~ 이런거 입니다.
  - 메모리 할당: A 프로세스가 할당된 영역에 `read` 또는 `write`할 때 실제 메모리를 할당해주는 것입니다. (지연 할당)

<br>

### 4-1. 메모리 영역 할당

#### 예제 코드

```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <string.h>

#define ALLOC_MB 100  // 100MB 할당

int main() {
    size_t size = ALLOC_MB * 1024 * 1024;
    printf("🔹 Step 1: malloc으로 %dMB 영역만 할당합니다.\n", ALLOC_MB);
    
    char *buffer = malloc(size);
    if (!buffer) {
        perror("malloc");
        return 1;
    }

    printf("🔸 현재 메모리 상태를 확인해보세요 (다른 터미널에서):\n");
    printf("    > cat /proc/%d/status | grep Vm\n", getpid());
    printf("    > pmap %d | grep anon", getpid());
    printf("\n👉 엔터를 누르면 메모리를 실제로 접근합니다...\n");
    getchar();

    printf("🔹 Step 2: memset으로 메모리에 실제 접근을 시작합니다.\n");
    memset(buffer, 0, size);  // 실제로 메모리에 쓰기 (페이지 폴트 발생)

    printf("🔸 다시 메모리 상태를 확인해보세요 (Resident size가 늘어남):\n");
    printf("    > cat /proc/%d/status | grep Vm\n", getpid());
    printf("    > pmap %d | grep anon", getpid());

    getchar(); // 종료 전 대기
    free(buffer);
    return 0;
}
```

<br>

#### 🚗 과정

- `VmRSS`는 실제 물리 메모리가 할당된 크기를 의미합니다. 즉 전체 할당된 메모리 크기(`VmSize`)중 현재는 772kB만 할당되어 있다는 것을 의미합니다.
- `memset()` 함수 호출 전 `VmRSS`는 772kB가 할당되어 있습니다. 이는 `메모리 영역 할당`은 가상 주소 공간에 자리만 확보했다는 것을 의미합니다.
- `pmap` 명령어를 통해 `mmap` system call은 가상 주소 공간에 자리만 확보했다는 것을 알 수 있습니다.

```txt
> cat /proc/1975/status | grep Vm
VmPeak:	  104584 kB
VmSize:	  104584 kB
VmLck:	       0 kB
VmPin:	       0 kB
VmHWM:	     772 kB
VmRSS:	     772 kB << 확인
VmData:	  102608 kB
VmStk:	     132 kB
VmExe:	       4 kB
VmLib:	    1744 kB
VmPTE:	      44 kB
VmSwap:	       0 kB

> pmap 1975 | grep anon
0000aaaac8119000    132K rw---   [ anon ]
0000ffff8b170000 102400K rw---   [ anon ] << mmap system call을 통해 확보한 가상 메모리(100MB)
0000ffff9170d000     48K rw---   [ anon ]
0000ffff91755000      8K rw---   [ anon ]
0000ffff91757000      8K r----   [ anon ]
0000ffff91759000      4K r-x--   [ anon ]

> 엔터 후 다시

> cat /proc/1975/status | grep Vm
VmPeak:	  104584 kB
VmSize:	  104584 kB
VmLck:	       0 kB
VmPin:	       0 kB
VmHWM:	  103668 kB
VmRSS:	  103668 kB << 확인
VmData:	  102608 kB
VmStk:	     132 kB
VmExe:	       4 kB
VmLib:	    1744 kB
VmPTE:	     240 kB
VmSwap:	       0 kB

> pmap 1975 | grep anon
0000aaaac8119000    132K rw---   [ anon ]
0000ffff8b170000 102400K rw---   [ anon ]
0000ffff9170d000     48K rw---   [ anon ]
0000ffff91755000      8K rw---   [ anon ]
0000ffff91757000      8K r----   [ anon ]
0000ffff91759000      4K r-x--   [ anon ]
```

<br>

### 4-2. 메모리 할당

- 메모리 영역 할당에서는 가상 메모리 공간만 확보했을 뿐 물리 메모리는 확보되지 않았습니다. 하지만 `메모리 할당` 과정에서는 실제로 물리 메모리가 할당되게 됩니다.

#### 🚗 과정

- 프로세스가 가상 주소를 통해 메모리에 접근하려고 시도합니다.
- 하지만 해당 주소는 아직 물리 메모리에 매핑되지 않았기 때문에 `페이지 폴트`가 발생하게 됩니다.
- 이때 커널은 `페이지 폴트 핸들러`를 호출하여 물리 페이지 프레임을 할당하고 가상 주소와 매핑시켜 줍니다.
- 이후 CPU가 다시 명령을 재실행하여 정상적인 메모리로 접근할 수 있도록 합니다.
- 메모리 영역 할당의 예제에서 `VmRSS`이 772KB에서 103668KB로 증가되었을 때 `메모리 할당`이 되었다는 것을 알 수 있습니다.

<br>

## 5. 페이지 테이블 계층화

#### 32비트 주소체계

- 32비트 주소 공간(4GB)을 4KB로 관리하려면 총 1024개의 페이지 테이블이 필요하고, 페이지 테이블의 용량은 4MB이며, 페이지 디렉토리는 4KB가 필요합니다.

#### 64비트 주소쳬게

- 이론상 64비트 주소체계는 18EB의 메모리 주소를 다룰 수 있다고 합니다. 하지만 이는 너무 큰 메모리여서 64비트 전체를 사용하지 않고 48비트만 사용하고 나머지 16비트는 CPU가 자동으로 0또는 1을 채운다고 합니다.
- 64비트 주소체계에서는 4단계 페이지 테이블로 나눠서 관리한다고 합니다.

### Huge Page

#### 📌 왜 등장하게되었을까?

- 128GB의 물리 메모리를 사용하고 있을 때 페이지의 크기가 4KB라면 약 3300만개 페이지를 만들게 됩니다.
- 만들어진 페이지를 페이지 테이블에 저장하고, `TLB`에도 저장이 됩니다. 그럼 가상 주소를 물리 주소로 변환활 때 `TLB`를 거치게 되는데 `TLB MISS`가 너무 발생하게 됩니다. 결국 TLB MISS가 발생하니 물리 메모리까지 접근하는 오버헤드가 증가하게 되고 성능이 낮아집니다.

#### 💡 Huge Page

- `Huge Page`는 4KB 대신 2MB/1GB와 같은 페이지를 사용하여 `TLB MISS`를 줄여줍니다.
- 128GB의 물리 메모리를 사용하고 있을 때 페이지 테이블의 크기가 2MB라면 6만 5천개의 페이지를 만들게 되고 `TLB MISS`가 줄어들어 성능이 향상됩니다.

<br>

### Transparent Huge Page(THP)

- ㅊ는 운영체제가 자동으로 `Huge Page`를 관리해주는 기능입니다. 기존에는 개발자가 직접 mmap() 함수의 MAP_HUGETLB 플래그를 사용하여 `Huge Page`를 요청했어야 했는데, `THP`는 가상 주소 공간 내부에 얀속된 4KB가 어떤 조건을 만족하면 그걸 하나로 묶어서 자동으로 `Huge Page`를 만들어주는 기능입니다.
- 단점으로는 여러 페이지를 합쳐서 하나의 `Huge Page`를 만드는데 들어가는 비용과 `Huge Page`를 구성하던 조건이 성립하지 않을 때 커다란 페이지를 다시 4KB 페이지로 다시 분할하는 비용이 듭니다.

#### 💡 시스템 설정 보기

- always: 가능한 한 항상 THP 적용
- madvise: 명시적으로 요청한 영역에만 적용
- never: THP 비활성화

```txt
> cat /sys/kernel/mm/transparent_hugepage/enabled
always [madvise] never
```

<br>


#### 참고

- https://github.com/kdg0209/realizers/blob/main/self-learning-cs/14%EC%9E%A5%20%EA%B0%80%EC%83%81%20%EB%A9%94%EB%AA%A8%EB%A6%AC.md




