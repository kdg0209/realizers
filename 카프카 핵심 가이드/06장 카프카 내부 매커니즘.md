# 카프카 내부 매커니즘

<br>

### 1. 클러스터 멤버십

- 브로커는 시잘될때마다 주키퍼에 Ephemeral Node 형태로 아이디를 등록하며, 동일한 아이디가 있으면 오류가 발생합니다.
- Ephemeral Node란 주키퍼와 연결된 동안에만 존재하는 임시 노드입니다. 브로커가 주키퍼와 연결이 끊어지면 이 임시 노드는 삭제됩니다.
- 주키퍼의 /bokers/ids 경로에는 브로커들의 아이디가 있으며 이 경로를 구독함으로써 브로커가 추가되는지 삭제되는지 알 수 있습니다.

<br>

### 2. 컨트롤러 브로커

- 브로커 컨트롤러는 일반적인 브로커로서의 역할도 수행하지만 여기에 더해 파티션 리더를 선출하는 역할도 수행합니다.
- 클러스터에서 가장 먼저 시작되는 브로커가 주키퍼에 Ephemeral Node를 등록함으로써 컨트롤러 브로커가 됩니다.
- 브로커가 클러스터를 나갔다는 사실을 인지하면 컨트롤러 브로커는 해당 브로커가 맡고 있던 모든 파티션을 새로운 브로커에게 할당해줍니다. 이때 새로운 파티션 리더 브로커가 만들어집니다.
- 컨트롤러 브로커는 zookeeper.session.timeout.ms에 설정된 시간보다 더 오랫동안 주키퍼에 하트비트를 보내지 않으면 임시 노드(Ephemeral Node)에서 제거됩니다. 그리고 클러스터 안에 있는 다른 브로커들은 주키퍼와 연동된 와처를 통해 컨트롤러 브로커가 없어졌다는 것을 알게되고, 주키퍼의 컨트롤러 노드가 되려고 시도하는데 가장 먼저 등록하는 브로커가 컨트롤러 브로커가 됩니다.

<br>

### 3. 복제

- 복제가 중요한 이유는 개별적인 노드에 필연적으로 장애가 발생할 수 밖에 없는 상황에서 카프카가 신뢰성과 지속성을 보장하는 방식입니다.

#### 💡 리더 레플리카(리더 파티션)

- 파티션에는 리더 역할을 하는 레플리카(리더 파티션) 프로듀셔와 컨슈머는 보통 리더 파티션으로부터 메시지를 쓰거나 읽습니다.
- 프로듀셔는 메시지 전송시 모든 레플리카에게 메시지를 전송하는게 아니라 리더 파티션에게만 메시지를 전송합니다. 그리고 컨슈머는 리더 파티션으로부터만 메시지를 읽습니다.
- 카프카 2.4버전부터는 컨슈머도 팔로워 파티션으로부터 메시지를 가져올 수 있습니다.(Follower Fetching)
  - 이 기능의 주요 목표는 클라이언트가 리더 파티션 대신 가장 가까이에 있는 ISR로부터 읽게 함으로써 네트워크 트레픽 비용을 줄이는 것입니다. 이 기능을 사용하기 위해서는 client.rack 컨슈머 값을 설정해야 합니다. 브로커 설정 중에서는 replica.selector.class 설정 필요
  - https://www.conduktor.io/kafka/kafka-topic-replication/#acks-=-all-2
  - https://developers.redhat.com/blog/2020/04/29/consuming-messages-from-closest-replicas-in-apache-kafka-2-4-0-and-amq-streams#
   
#### 💡 팔로워 레플리카(팔로워 파티션)

- 파티션에 속한 레플리카 중에서 리더 레플리카를 제외한 나머지 레플리카를 팔로워 레플리카라 합니다.
- 팔로워 파티션의 주된 역할은 리더 파티션으로부터 메시지를 복제함으로써 동기화를 잘 하는 것입니다. 만약 리더 파티션에 장애가 발생할 경우 ISR에 속한 팔로워 파티션 중 하나가 리더 파티션으로 승격됩니다.

#### 💡 선호 리더

- 선호 리더란 토픽이 처음 생성되었을 때 리더 파티션이였던 브로커를 가리킵니다. 파티션이 처음 생성되던 시점에서는 리더 파티션이 모든 브로커에 걸쳐 균등하게 분포되기 때문에 '선호'라는 이름이 붙었으며, 결과적으로 선호 리더가 실제 리더가 될 경우 부하가 균등하게 분배될 것이라 예측할 수 있습니다.
- 브로커에는 auto.leader.rebalance.enable 속성값이 true인데, 이 설정은 선호 리더가 현재 리더는 아니지만 ISR에서 동기화가 제대로 이루어지고 있을 때 선호 리더를 현재 리더로 만들어 줍니다.

#### 💡 리더 파티션으로부터 소비

- 리더 파티션에 존재하는 모든 데이터를 컨슈머가 읽을 수 있는 것은 아닙니다. 대부분 ISR에 쓰여진 메시지들만 읽을 수 있습니다.
- 특정 메시지는 ISR 그룹에 속해 있는 팔로워 파티션에 커밋되기 전까지 컨슈머들이 읽을 수 없습니다. 그 이유는 충분한 수의 팔로워 파티션에 복제가 완료되지 않은 메시지는 '불안전한 메시지'로 간주합니다.
- 예를들어 컨슈머가 리더 파티션으로부터 메시지를 읽은 상태에서 리더 파티션에 장애가 발생하여 팔로워 파티션이 리더로 승격되었는데, 승격된 파티션이 해당 메시지를 복제한적 없다면 메시지들 사이에 불일치가 발생하게 됩니다. 그렇기 때문에 ISR에 속한 팔로워들이 메시지를 모두 받을 때까지 기다린 뒤에 컨슈머가 소비할 수 있습니다.

<br>

### 4. 리더 파티션과 팔로워 파티션의 복제 과정

#### 🚗 과정

<p> [1 Flow] 프로듀서로부터 새로운 메시지를 받게 됩니다. 팔로워 파티션들은 리더 파티션으로부터 메시지를 복제하기 위해 Fetch요청을 보낸 후 새로운 메시지가 있다는 것을 알게된 후 복제하게 됩니다. </p>
<p> [2 Flow] 리더 파티션은 모든 팔로워 파티션들이 0번째 오프셋 메시지에 대해 Fetch요청을 보냈다는 것을 알고 있습니다. 그러나 리더 파티션은 팔로워 파티션들이 성공적으로 복제했는지는 아직 모릅니다. </p>
<p> [3 Flow] 프로듀서로부터 새로운 메시지를 받게됩니다. </p>
<p> [4 Flow] 0번 오프셋에 대한 복제를 마친 팔로워 파티션들은 리더 파티션으로부터 또 Fetch요청을 보내게 됩니다. 이때 리더 파티션은 0번 오프셋이 정상적으로 복제되었음을 인지하게되고 오프셋 0번을 커밋하게 됩니다. 만약 팔로워 파티션이 0번 오프셋을 정상적으로 복제하지 못 했다면 팔로워는 다시 Fetch 요청을 보낼때 1번 오프셋이 아닌 0번 오프셋에 대해 Fetch하게 됩니다. 따라서 리더는 팔로워가 요청한 Fetch를 통해 잘 복제되고 있는지 파악할 수 있습니다. </p>
<p> [5 Flow] 팔로워들로부터 1번 오프셋에 대해 Fetch 요청을 받은 리더는 응답에 0번 오프셋 메시지가 커밋되었다는 내용도 함께 전달합니다. 그리고 리더의 응답을 받은 모든 팔로워들도 0번 오프셋 메시지가 커밋되었다는 것을 알고 커밋하게 됩니다. </p>
<img width="1337" alt="스크린샷 2025-01-15 오후 10 03 19" src="https://github.com/user-attachments/assets/5fdf0bfa-5fa2-4053-9bf5-3334045ed704" />
<img width="1337" alt="스크린샷 2025-01-15 오후 10 02 49" src="https://github.com/user-attachments/assets/0d3d6498-a195-48e1-a49c-94e43fe3afa6" />

<br>

### 5. Isr(In-Sync-Replica)

- 리더 파티션과 팔로워 파티션은 ISR이라는 논리적 그룹으로 묶여 있습니다.
- 논리적 그룹으로 나눈 이유는 해당 그룹에 속한 팔로워 파티션들만이 리더 파티션이 죽은 경우 리더의 자격을 가질 수 있기 때문입니다. 다시말해 ISR 그룹에 속하지 못한 팔로워들은 리더가 될 수 없습니다.
- 복제 과정을 통해 ISR내에 있는 모든 팔로워들에 복제가 완료되면 리더는 내부적으로 커밋되었다는 표시를 하게되었는데, 이때 마지막 오프셋 위치는 '하이 워터 마크'라 불립니다.

<br>

### 6. 로그 세그먼트

- 프로듀서로부터 전달받은 메시지는 브로커의 segment라는 파일에 저장됩니다.
- 세그먼트의 기본 크기는 1GB로 설정되어 있으며, 세그먼트의 크기가 1GB보다 커지는 경우 세그먼트는 닫히게 됩니다. 그리고 롤링 전략이 적용되게 됩니다.
- 닫힌 세그먼트에 대해서는 읽기만 가능하고, 현재 쓰이지고 있는 세그먼트를 액티브 세그먼트라 합니다.

![스크린샷 2025-01-16 오후 9 02 19](https://github.com/user-attachments/assets/42169683-d8bc-4e7d-a242-8eacbab2a76b)

<br>

#### 💡 롤링 매커니즘

```bash
1. 예제 토픽 생성
> kafka-topics --bootstrap-server localhost:9092 --create --topic segment-test-topic --partitions 3

2. 생성한 토픽의 segment byte 수정
> kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name segment-test-topic --alter --add-config segment.bytes=10240

3. 변경한 내용 확인
> kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name segment-test-topic --all --describe | grep segment.bytes

4. 아래와 같이 출력되면 정상적으로 변경된 것
segment.bytes=10240 sensitive=false synonyms={DYNAMIC_TOPIC_CONFIG:segment.bytes=10240, STATIC_BROKER_CONFIG:log.segment.bytes=1073741824, DEFAULT_CONFIG:log.segment.bytes=1073741824}

5. 테스트 메시지 파일 작성
for i in {1..2000}
do
echo "test message send test-i" >> message.log
done

6. 파일 기반으로 메시지 전송
> kafka-console-producer --bootstrap-server localhost:9092 --topic segment-test-topic < message.log
```

![스크린샷 2025-01-16 오후 9 09 29](https://github.com/user-attachments/assets/e851fda9-e277-4537-bfb8-ae7792060226)

<br>

### 7. Log Cleanup Policy

#### delete

- delete로 설정하면 세그먼트를 log.retention.hours나 log.retention.bytes 설정값에 따라 삭제합니다.

#### compact

- compact로 설정하면 세그먼트를 각 키의 가장 최근값만 저장하여 세그먼트를 재구성합니다.
- 만약 메시지의 키가 NULL인 경우 압착은 실패합니다. 즉, 키가 NULL인 메시지는 적용할 수 없습니다.
- log cleaner가 백그라운드 스레드 방식으로 별도의 I/O 작업을 수행하므로 추가적인 I/O 부하가 소모됩니다.
- log compaction을 사용하면 빠른 장애 복구를 할수 있습니다. 장애 복구 시 전체 로그를 복구하지 않고, 메시지의 키를 기준으로 최신 상태만 복구하며, 따라서 전체 로그를 복구할 때보다 복구 시간을 줄일 수 있습니다.
- compact 사용시 특정 키를 완전히 삭제하려면 특정 키의 값을 NULL로 세팅하면됩니다. 그럼 클리너 스레드가 해당 메시지를 발견하면 NULL값을 가지는 메시지는 '톰스톤'으로 마킹해놓고 나중에 톰스톤 메시지를 삭제하게 되고, 해당 키 역시 파티션에서 완전히 삭제하게 됩니다.
- compact 역시 현재 액티브 세그먼트는 절대로 압착하지 않고, close된 세그먼트에 대해서만 압착의 대상이 됩니다.

![스크린샷 2025-01-16 오후 9 24 24](https://github.com/user-attachments/assets/5813f189-4331-4abf-8392-eda75c429046)



