# 카프카 설치하기

<br>

## 1. 카프카 구성하기

#### 1-1. ubuntu 22.04 버전으로 컨테이너 생성

```bash
docker run -it --name kafka-container -p 9092:9092 --privileged=true ubuntu:22.04
```

#### 1-2. JDK-17 버전 다운로드

```bash
> apt-get update && upgrade
> apt-get install openjdk-17-jdk
> java --version
```

#### 1-3. kafka 설치

```bash
> cd home
> apt-get install wget
> wget https://packages.confluent.io/archive/7.8/confluent-community-7.8.0.tar.gz
> tar -xzvf confluent-community-7.8.0.tar.gz
> rm -rf confluent-community-7.8.0.tar.gz
```

#### 1-4. 환경 변수 지정

```bash
> cd ~
> apt-get install vim
> vi .bashrc
> export CONFLUENT_HOME=/home/confluent-7.8.0
> export PATH=.:$PATH:$CONFLUENT_HOME/bin
>  wq!
>  . .bashrc (수정 내역 적용)
> echo $CONFLUENT_HOME 테스트 입력
/home/confluent-7.8.0 < 출력되어야 함
```

#### 1-5. 주키퍼 및 카프카 테스트 실행

```bash
> zookeeper-server-start $CONFLUENT_HOME/etc/kafka/zookeeper.properties
> kafka-server-start $CONFLUENT_HOME/etc/kafka/server.properties
```

#### 1-6. 주키퍼 및 카프카 로그 디렉토리 설정

- 아래 설정을 한 뒤 주키퍼와 카프카를 실행시켜 디렉토리가 잘 잡혔는지 확인해야 합니다.

```bash
> cd home
> mkdir data
> mkdir zookeeper
> mkdir kafka-logs
> cd $CONFLUENT_HOME/etc/kafka
> vi server.properties
> log.dirs=/home/data/kafka-logs 변경 뒤 저장
> vi zookeeper.properties
> dataDir=/home/data/zookeeper 변경 뒤 저장
```

#### 1-7. 간편 쉘 스크립트 등록

```bash
> cd home
> vi zoo_start.sh
> $CONFLUENT_HOME/bin/zookeeper-server-start $CONFLUENT_HOME/etc/kafka/zookeeper.properties < 작성 후 저장
> vi kafka_start.sh
> $CONFLUENT_HOME/bin/kafka-server-start $CONFLUENT_HOME/etc/kafka/server.properties < 작성 후 저장
> chmod +x *.sh
> ./zoo_start.sh   # 주키퍼 실행
> ./kafka_start.sh # 카프카 브로커 실행
```

#### 1-8. 토픽 생성 테스트

```bash
> kafka-topics --bootstrap-server localhost:9092 --create --topic welcome-topic
Created topic welcome-topic # 뜨면 정상적으러 만들어진것
```

##### 참고

- 로컬에서 프로듀서 서버가 카프카(브로커)에 접속하기 위해서는 server.properties의 dvertised.listeners를 수정해야함

```bash
#advertised.listeners=PLAINTEXT://your.host.name:9092 
advertised.listeners=PLAINTEXT://localhost:9092 << 이거처럼 수정
```

<br>

## 2. 브로커 설정하기

#### broker.id

- 모든 카프카 브로커들은 정수값의 식별자를 가집니다. 기본값은 0이지만 어떠한 값도 될 수 있으며, 이 값은 클러스터안에서 고유해야 합니다.

#### listeners 

- listeners 설정의 예로는 PLAINTEXT://localhost:9092, SSL://9091이 있으며, 호스트 이름을 0.0.0.0으로 설정하면 모든 네트워크 인터페이스로부터 열결을 받게 됩니다. 만약 이 값을 비우면 기본 인터페이스에 대해서만 연결을 받습니다.
- 1024 미만의 포트 번호를 사용할 경우 root 권한이 필요하며, 이러한 설정은 바람직하지 않습니다.

#### zookeeper.connect

- 주키퍼와의 연결을 설정합니다.
- 기본은 localhost:2181입니다.

#### log.dirs

- 카프카는 메시지를 로그 세그먼트 단위로 묶어서 log.dirs에 설정된 디렉토리에 메시지를 저장합니다. 

#### num.recovery.threads.per.data.dir

- 기본적으로 하나의 로그 디렉토리에 하나의 스레드만이 사용됩니다. 이 스레드들은 브로커가 시작되고 종료될 때만 사용되기 때문에 작업을 병렬화하기 위해서는 많은 수의 스레드를 할당해주는게 좋습니다. 특히 브로커에 많은 파티션이 있는 경우 성능 향상을 할 수 있습니다.
- 만약 'num.recovery.threads.per.data.dir' 설정이 1이고 log.dirs에 지정된 경로가 3개라면 전체 스레드 수는 3개입니다.
- 카프카는 스레드 풀을 사용하여 로그 세그먼트를 관리합니다. 이 스레드 풀은 아래와 같은 작업을 수행합니다.
  1. 브로커가 정상적으로 시작되었을 때 각 파티션의 로그 세그먼트를 엽니다.
  2. 브로커가 장애 발생 후 재시작되었을 때 각 파티션의 로그 세그먼트를 검사하고 잘못된 부분은 삭제합니다.
  3. 브로커가 종료할 때 로그 세그먼트를 닫습니다.

#### auto.leader.rebalance.enable

- 브로커에 장애가 발생하여 하나의 브로커가 많은 leader partition를 차지하는 경우 불균형이 발생할 수 있습니다. 이때 auto.leader.rebalance.enable의 값을 TRUE로 설정하면 백그라운드에서 별도의 스레드가 리더의 불균형을 주기적으로 확인하게 됩니다.
- leader.imbalance.check.interval.seconds의 값으로 컨트롤러가 주기적으로 확인하게되며, 전체 파티션 중 특정 브로커에 리더 역할이 할당된 파티션 비율이 leader.imbalance.per.broker.percentage에 설정된 값을 넘어가면 선호 리더(preferred leader) 리밸런싱이 발생하게 됩니다.
- bin 디렉토리에 위치한 kafka-leader-election.sh 스크립트를 실행하면 선호 리더에 맞게 리더가 재선정되어 불균형을 해소할 수 있습니다.

#### num.partitions

- 토픽 생성시 파티션 수를 지정하지 않으면 기본적으로 1입니다. 파티션 수는 증가시킬 수 있지만 감소할 수 없기 때문에 유의해야 합니다.

#### default.replication.factor

- 해당 설정은 토픽의 복제 펙터 수를 지정할 수 있습니다.
- default.replication.factor의 값은 min.insync.replicas 설정보다 최소한 1이상 잡아줄 것을 권고하고 있습니다.
- default.replication.factor의 값과 min.insync.replicas의 값은 동일해서는 안되며 default.replication.factor 값이 항상 더 커야 안전합니다.
- 만약 카프카 브로커가 3대가 있고 토픽의 파티션 수가 3이라는 가정
  - default.replication.factor = 3으로 설정을 하여 브로커 3대에게 메시지를 복제합니다.
  - min.insync.replicas = 3으로 설정되어 있는 상태에서 만약 브로커 한대가 장애가 발생하면 min.insync.replicas = 3 설정으로 인해 클러스터 전체에 문제가 발생할 가능성이 매우 높습니다.
  - 따라서 가장 베스트 설정은 min.insync.replicas = 2로 설정하는게 맞다고 생각합니다.
 
#### min.insync.replicas 

- min.insync.replicas 설정은 리더 파티션이 복제본을 몇개의 레플리카로 전달할지에 대한 속성입니다.

#### log.retention.ms

- 기본값은 log.retention.hour 설정을 사용하며 168시간(일주일) 입니다.
- log.retention.minutes,log.retention.ms 설정도 사용될 수 있으며, log.retention.ms 설정을 사용하는 것이 권장됩니다. 그 이유는 더 작은 단위의 설정값이 우선권을 가지기 때문입니다.

#### log.retention.bytes

- 메시지 만료의 또 다른 기준은 메시지의 용량입니다. 이 설정은 파티션 단위로 적용됩니다.
- 만약 8개의 파티션을 가진 토픽에 log.retention.bytes 설정값이 1GB로 되어 있다면 토픽의 최대 용량은 8GB입니다.

#### log.retention.ms와 log.retention.bytes

- 두 설정이 모두 되어 있다면 두 조건 중 하나만 성립해도 메시지가 삭제될 수 있습니다. 예를들어 log.retention.ms의 설정이 1일로 되어있고, log.retention.bytes 설정이 1GB로 되어 있는 경우 메시지의 크기가 1GB가 넘어가면 저장된지 하루가 안되더라도 삭제되는 것입니다. 반대도 마찬가지 입니다.

#### log.segment.byte

- 로그 세그먼트의 크기가 log.segment.byte에 지정된 크기에 다다르면(기본값: 1GB), 브로커는 기존 로그 세그먼트를 닫고 새로운 세그먼트를 엽니다.
- 로그 세그먼트는 닫히기 전까지는 만료와 삭제의 대상이 되지 않습니다.

#### log.roll.ms

- 로그 세그먼트 파일이 닫히는 시점을 제어하는 또 다른 방법은 log.roll.ms 설정값을 사용하는 것인데 이 방법은 권장되지 않습니다.
- 예를들어 로그 세그먼트가 많을 경우 시간을 기준으로 한다면 동시에 닫힐 때 디스크 성능에 대한 영향이 있기 때문입니다.

#### message.max.bytes

- 메시지의 최대 크기입니다. 기본값은 1GB이며, 프로듀서가 해당 설정보다 큰 메시지를 보내면 에러가 발생합니다.
- message.max.bytes 설정은 컨슈머 클라이언트의 fetch.message.max.bytes 설정과 맞아야 합니다. 만약 이 값이 message.max.bytes 설정보다 작을 경우 컨슈머가 더 큰 메시지를 읽는데 실패하고 에러가 발생합니다.


