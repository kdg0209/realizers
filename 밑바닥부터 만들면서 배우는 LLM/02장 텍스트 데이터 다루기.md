## 1. 단어 임베딩 이해하기

- LLM을 포함해 심층 신경망 모델은 텍스트를 이해할 수 없기 때문에 텍스트를 이해할 수 있도록 실수 벡터 형태로 만들어야 하는데, 텍스트를 실수 벡터 형태로 변환하는 과정을 `임베딩`이라 합니다.
- 데이터의 형식마다 고유한 임베딩 모델이 필요로 하는데 예를들어 텍스트를 위한 임베딩 모델은 오디오나 비디오 데이터를 임베딩하는 데는 적합하지 않습니다.

#### 임베딩의 목적

- 비수치 데이터를 신경망이 처리할 수 있는 포멧으로 변환하는 것입니다.
- 임베딩 차원은 128, 1024 등 다차원에 저장이 가능한데 차원이 높을수록 미묘한 관계를 잘 감지할 수 있지만 계산 효율성이 떨어집니다.

<br><br>

## 2. 텍스트 토큰화하기

- 임베딩을 만드는데 필수적인 전처리 단계인, 입력 텍스트를 개별 토큰으로 분할하는 작업에 대해 알아보는데, 여기서는 단순히 정규화를 통해 입력 텍스트를 개별 토큰으로 분할합니다.
- [예제 코드](https://github.com/rickiepark/llm-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb)

```python
import re

with open("./the-verdict.txt", "r", encoding="utf-8") as f:
    raw_text = f.read()
preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)', raw_text)
preprocessed = [item.strip() for item in preprocessed if item.strip()]
print(preprocessed[:30])

아웃풋
['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']
```

<br><br>

## 3. 토큰을 토큰 ID로 변환하기

- 이 과정은 `토큰 ID를 임베딩 벡터로 변환하기 전의 중간 단계`입니다.

### 과정
<img width="1032" height="395" alt="스크린샷 2025-11-23 오후 2 16 17" src="https://github.com/user-attachments/assets/434f0d6f-e88f-46fc-bb6e-4c3bdaa701ce" />

<br>

### 코드

- 아래 코드는 `the-verdict.txt` 파일의 내용을 개별 토큰으로 나누고, 어휘사전을 구성한 뒤 특정 문자열을 `tokenizer.encode`을 사용하여 텍스트를 토큰 ID로 변환합니다. `tokenizer.decode`을 사용하여 토큰 ID를 다시 문자열로 변환하는 예제입니다.
- 하지만 text를 Hello, LLM으로 테스트하면 the-verdict.txt 파일에는 Hello가 없기 때문에 키 매핑이 되지않아 오류가 발생합니다.

```python
import re

class TokenizerV1:
    def __init__(self, vocab):
        self.text_to_int = vocab
        self.int_to_text = {i:s for s, i in vocab.items()}
        
    def encode(self, text):
        preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)', text)
        preprocessed = [
            item.strip() for item in preprocessed if item.strip()
        ]

        ids = [self.text_to_int[s] for s in preprocessed]
        return ids
    
    def decode(self, ids):
        text = " ".join([self.int_to_text[i] for i in ids])
        text = re.sub(r'\s+([,.?!"()\'])', r'\1', text)
        return text


with open("./the-verdict.txt", "r", encoding="utf-8") as f:
    raw_text = f.read()
preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)', raw_text)
preprocessed = [item.strip() for item in preprocessed if item.strip()]

all_words = sorted(set(preprocessed))
vocab_size = len(all_words)
vocab = {token:integer for integer,token in enumerate(all_words)}

tokenizer = TokenizerV1(vocab)

text = "It's the last he painted, you know, Mrs. Gisburn said with pardonable pride."
ids = tokenizer.encode(text)
print(ids) # [56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 67, 7, 38, 851, 1108, 754, 793, 7]
print(tokenizer.decode(ids)) # It' s the last he painted, you know, Mrs. Gisburn said with pardonable pride.

// 오류 발생(the-verdict.txt 파일에 Hello 단어가 없기 때문에 키 매핑 오류)
text = "Hello, LLM"
ids = tokenizer.encode(text)
print(ids) 
print(tokenizer.decode(ids))
```

<br><br>

## 3. 특수 문맥 토큰 추가하기

- 모델이 텍스트로부터 문맥이나 그 밖의 관련된 정보를 잘 이해할 수 있도록 특수 문맥 토큰도 추가해야하는데, 특수 토큰은 알지 못하는 단어, 문서 경계 등을 표시하는데 사용됩니다.

<br>

### 토큰 종류

#### [BOS] 

- 이 토큰은 텍스트의 시작을 표시합니다.

#### [EOS] 

- 이 토큰은 텍스트의 끝에 위치하며 `<|endoftext|>`와 비슷하게 관련 없는 여러 개의 텍스트를 연결할 때 유용합니다. 또는 서로 다른 2개의 문서를 합칠 때 사용되기도 합니다.

#### [PAD]

- 하나 이상의 배치 크기로 LLM을 훈련할 때 배치 안에 길이가 다른 텍스트가 포함될 수 있는데, 모든 텍스트의 길이를 동일하게 맞추기 위해 짧은 텍스트 토큰을 사용해 길이를 맞출때 사용됩니다.
- [PAD 예제](https://github.com/kdg0209/realizers/blob/main/LLM%EC%9D%84%20%ED%99%9C%EC%9A%A9%ED%95%9C%20%EC%8B%A4%EC%A0%84%20AI%20%EC%95%A0%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98%20%EA%B0%9C%EB%B0%9C/02%EC%9E%A5%20%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98.md#-%EB%B0%B0%EC%B9%98-%EC%A0%95%EA%B7%9C%ED%99%94)

<br>

### 코드

```python
import re

class TokenizerV2:
    def __init__(self, vocab):
        self.text_to_int = vocab
        self.int_to_text = {i:s for s, i in vocab.items()}
        
    def encode(self, text):
        preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)', text)
        preprocessed = [
            item.strip() for item in preprocessed if item.strip()
        ]

        # 알지 못하는 단어를 <|unk|> 토큰으로 변환
        preprocessed = [item if item in self.text_to_int else "<|unk|>" for item in preprocessed]

        ids = [self.text_to_int[s] for s in preprocessed]
        return ids
    
    def decode(self, ids):
        text = " ".join([self.int_to_text[i] for i in ids])
        text = re.sub(r'\s+([,.:;?!"()\'])', r'\1', text) # 구두점 문자 앞의 공백 삭제
        return text

with open("./the-verdict.txt", "r", encoding="utf-8") as f:
    raw_text = f.read()
preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)', raw_text)
preprocessed = [item.strip() for item in preprocessed if item.strip()]

all_tokens = sorted(list(set(preprocessed)))
all_tokens.extend(["<|endoftext|>", "<|unk|>"])
vocab = {token:integer for integer,token in enumerate(all_tokens)}

text1 = "Hello, do you like tea?"
text2 = "In the sunlit terraces of the palace."
text = " <|endoftext|> ".join((text1, text2))

tokenizer = TokenizerV2(vocab)
print(tokenizer.decode(tokenizer.encode(text))) # <|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.
```

<br><br>

## 4. 바이트 페어 인코딩

- 바이트 페어 인코딩(`BPE`)는 GPT-2, GPT-3, ChatGPT에 사용되었습니다.
- `tiktoken` 라이브러리 OpenAI에서 제공하는 토큰 관리 라이브러리로, 텍스트를 토큰 단위로 분할하고, 각 토큰을 효율적으로 관리할 수 있도록 도와줍니다. 이 패키지는 특히 GPT 모델 시리즈와 호환되도록 설계되었습니다.

```python
import tiktoken

# GPT2로 모델 설정
encoding = tiktoken.get_encoding("gpt2")

text = "Hello LLM? Do you like tea?"

# 텍스트를 토큰화
tokens = encoding.encode(text)
print(f"Tokens: {tokens}") # [15496, 27140, 44, 30, 2141, 345, 588, 8887, 30]

# 토큰 수 계산
num_tokens = len(tokens)
print(f"Number of tokens: {num_tokens}") # 9

# 토큰을 다시 텍스트로 디코딩
decoded_text = encoding.decode(tokens)
print(f"Decoded text: {decoded_text}") # Hello LLM? Do you like tea?
```

<br><br>

## 5. 토큰 임베딩하기

- 텍스트를 토큰화하고, 토큰을 토큰 ID로 바꾸고 토큰 ID를 임베딩 벡터로 변환하는 과정을 살펴봅니다.

### 단계별 살펴보기

#### embedding_layer.weight

- vocab_size: 6, embedding_size: 3은 torch.nn.Embedding(6, 3)이므로 각 row가 하나의 토큰의 임베딩 벡터입니다.

#### embedding_layer(input_ids)

- 2번 토큰, 3번 토큰, 5번 토큰, 1번 토큰의 임베딩 벡터를 차례대로 출력하라는 의미입니다.
  - embedding_layer.weight[2], embedding_layer.weight[3], embedding_layer.weight[5], embedding_layer.weight[1] 이렇게 출력한거와 동일합니다.
  - input_ids[0] == 2 → 3번째 행: [ 1.2753, -0.2010, -0.1606]
  - input_ids[1] == 3 → 4번째 행: [-0.4015, 0.9666, -1.1481]
  - input_ids[2] == 5 → 6번째 행: [-2.8400, -0.7849, -1.4096]
  - input_ids[3] == 1 → 2번째 행: [ 0.9178, 1.5810, 1.3010]

```python
import torch
from torch.utils.data import Dataset, DataLoader

input_ids = torch.tensor([2, 3, 5, 1])
vocab_size = 6 # 어휘 사전 사이즈
embedding_size = 3 # 한 단어를 몇 차원 벡터로 표현할지
torch.manual_seed(123) # 랜덤 시드 123으로 지정
embedding_layer = torch.nn.Embedding(vocab_size, embedding_size)

print(embedding_layer.weight)
출력 결과
Parameter containing:
tensor([[ 0.3374, -0.1778, -0.1690],
        [ 0.9178,  1.5810,  1.3010],
        [ 1.2753, -0.2010, -0.1606],
        [-0.4015,  0.9666, -1.1481],
        [-1.1589,  0.3255, -0.6315],
        [-2.8400, -0.7849, -1.4096]], requires_grad=True)

print(embedding_layer(torch.tensor([3]))) 
출력 결과 (embedding_layer.weight의 4번째 행과 같은 값)
tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)

print(embedding_layer(input_ids))
출력 결과 
tensor([[ 1.2753, -0.2010, -0.1606],
        [-0.4015,  0.9666, -1.1481],
        [-2.8400, -0.7849, -1.4096],
        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)
```

<br><br>

## 6. 단어 위치 인코딩하기

- 트랜스포머는 사용자의 입렵을 병렬적으로 처리하는데, 그 과정에서 순서 정보가 사라지게 됩니다. 텍스트에서 순서는 매우 중요한 정보를 담당하기 때문에 어떻게든 순서 정보를 알아야하는데 이를 `위치 인코딩`이 처리해줍니다.
- 예를들어 "나는 밥을 먹었다"와 "먹었다 밥을 나는"을 구분할 수 없기 때문에 단어 순서 정보를 임의적으로 추가해줍니다.
- 절대 위치 임베딩과 상대 위치 임베딩이 있는데 모두 LLM이 토큰 사이의 순서와 관계를 이해하는 능력을 보강하여 정확하고 맥락을 고려한 예측을 만드는데 목적이 있습니다.

### 절대 위치 임베딩

- 절대 위치 임베딩은 시퀀스의 특정 위치에 직접 연관됩니다.
- 입력 시퀀스의 각 위치에 대해서 고유한 임베딩이 토큰 임베딩에 더해져 정확한 위치 정보를 추가합니다.

### 상대 위치 임베딩

- 상대 위치 임베딩은 상대적인 위치 또는 토큰 사이의 거리를 강조합니다.

<br>

## 정리

- 이 장에서는 입력 임베딩 파이프라인에 대해 전반적인 과정을 살펴보았습니다.
  1. 사용자의 입력을 받습니다. (ex: Hello World)
  2. 주어진 입력값(텍스트)을 토큰화합니다.
  3. 개별 토큰을 토큰 ID로 변환합니다.
  4. 문맥 정보를 잘 이해할 수 있도록 특수 문맥(BOS, EOS, PAD 등)을 토큰에 추가합니다.
  5. 임베딩을 생성합니다.
  6. LLM이 잘 처리할 수 있도록 위치 정보를 임베딩에 추가합니다.


